{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Import the required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "3pgSSSy-W659"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from mtcnn import MTCNN\n",
        "import tensorflow as tf\n",
        "from keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras import models, layers, optimizers\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "from keras import regularizers\n",
        "from keras.layers import Dropout\n",
        "import datetime\n",
        "import openpyxl\n",
        "from tensorflow.keras.models import load_model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Train CNN model using VGG-Face model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1934 images belonging to 19 classes.\n"
          ]
        }
      ],
      "source": [
        "#defining data set\n",
        "dataset_path=r\"C:\\Users\\marey\\OneDrive\\Documents\\Test_22\\data\"\n",
        "Train_Data=tf.keras.preprocessing.image.ImageDataGenerator(\n",
        "    horizontal_flip=True,\n",
        "    rescale=1/255.0,\n",
        ").flow_from_directory(dataset_path,batch_size=16,subset=\"training\",target_size=(224,224),shuffle=False)\n",
        "class_labels =list(Train_Data.class_indices.keys())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['AbdulAziz',\n",
              " 'Abdulrahman Khalid',\n",
              " 'Abdulrhman Younis',\n",
              " 'Afnan',\n",
              " 'Ahmed',\n",
              " 'Aishah',\n",
              " 'Anas',\n",
              " 'Hala',\n",
              " 'Hassan',\n",
              " 'Hazem',\n",
              " 'Mariyyah',\n",
              " 'Marwah',\n",
              " 'Muhammad Alhudari',\n",
              " 'Muhammed Dhabaab',\n",
              " 'Omar',\n",
              " 'Shahad Alnami',\n",
              " 'Shoog',\n",
              " 'Snd',\n",
              " 'Tariq']"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "class_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "- Train The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 1934 images belonging to 19 classes.\n",
            "Epoch 1/20\n",
            "47/47 [==============================] - 228s 5s/step - loss: 3.4155 - accuracy: 0.0948 - val_loss: 3.0646 - val_accuracy: 0.1514\n",
            "Epoch 2/20\n",
            "47/47 [==============================] - 214s 5s/step - loss: 2.8694 - accuracy: 0.2179 - val_loss: 2.6472 - val_accuracy: 0.2764\n",
            "Epoch 3/20\n",
            "47/47 [==============================] - 208s 4s/step - loss: 2.4858 - accuracy: 0.3333 - val_loss: 2.1771 - val_accuracy: 0.4303\n",
            "Epoch 4/20\n",
            "47/47 [==============================] - 216s 5s/step - loss: 2.1546 - accuracy: 0.4191 - val_loss: 1.9294 - val_accuracy: 0.4976\n",
            "Epoch 5/20\n",
            "47/47 [==============================] - 201s 4s/step - loss: 1.8754 - accuracy: 0.4874 - val_loss: 1.7173 - val_accuracy: 0.5577\n",
            "Epoch 6/20\n",
            "47/47 [==============================] - 199s 4s/step - loss: 1.8163 - accuracy: 0.5158 - val_loss: 1.5371 - val_accuracy: 0.6442\n",
            "Epoch 7/20\n",
            "47/47 [==============================] - 208s 4s/step - loss: 1.6231 - accuracy: 0.5667 - val_loss: 1.6974 - val_accuracy: 0.5673\n",
            "Epoch 8/20\n",
            "47/47 [==============================] - 213s 5s/step - loss: 1.4645 - accuracy: 0.6144 - val_loss: 1.4636 - val_accuracy: 0.6298\n",
            "Epoch 9/20\n",
            "47/47 [==============================] - 209s 4s/step - loss: 1.3907 - accuracy: 0.6460 - val_loss: 1.3463 - val_accuracy: 0.6587\n",
            "Epoch 10/20\n",
            "47/47 [==============================] - 203s 4s/step - loss: 1.2363 - accuracy: 0.6725 - val_loss: 1.5888 - val_accuracy: 0.5986\n",
            "Epoch 11/20\n",
            "47/47 [==============================] - 200s 4s/step - loss: 1.2613 - accuracy: 0.6718 - val_loss: 1.4437 - val_accuracy: 0.6731\n",
            "Epoch 12/20\n",
            "47/47 [==============================] - 202s 4s/step - loss: 1.3897 - accuracy: 0.6389 - val_loss: 1.4465 - val_accuracy: 0.6274\n",
            "Epoch 13/20\n",
            "47/47 [==============================] - 247s 5s/step - loss: 1.2268 - accuracy: 0.6847 - val_loss: 1.3788 - val_accuracy: 0.6611\n",
            "Epoch 14/20\n",
            "47/47 [==============================] - 285s 6s/step - loss: 1.0763 - accuracy: 0.7286 - val_loss: 1.3091 - val_accuracy: 0.6899\n",
            "Epoch 15/20\n",
            "47/47 [==============================] - 247s 5s/step - loss: 1.3158 - accuracy: 0.6589 - val_loss: 1.5067 - val_accuracy: 0.5889\n",
            "Epoch 16/20\n",
            "47/47 [==============================] - 267s 6s/step - loss: 1.0399 - accuracy: 0.7395 - val_loss: 1.2849 - val_accuracy: 0.6923\n",
            "Epoch 17/20\n",
            "47/47 [==============================] - 252s 5s/step - loss: 0.7938 - accuracy: 0.8124 - val_loss: 1.2884 - val_accuracy: 0.6971\n",
            "Epoch 18/20\n",
            "47/47 [==============================] - 241s 5s/step - loss: 0.6449 - accuracy: 0.8607 - val_loss: 1.2667 - val_accuracy: 0.6995\n",
            "Epoch 19/20\n",
            "47/47 [==============================] - 249s 5s/step - loss: 0.5158 - accuracy: 0.8949 - val_loss: 1.2033 - val_accuracy: 0.7188\n",
            "Epoch 20/20\n",
            "27/47 [================>.............] - ETA: 1:23 - loss: 0.4810 - accuracy: 0.9091WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 940 batches). You may need to use the repeat() function when building your dataset.\n",
            "47/47 [==============================] - 136s 3s/step - loss: 0.4810 - accuracy: 0.9091 - val_loss: 1.3384 - val_accuracy: 0.6827\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x294dc2f0dd0>"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Create an ImageDataGenerator for data preprocessing and augmentation\n",
        "data_generator = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "# Load the data and split into training and validation sets\n",
        "data = data_generator.flow_from_directory(dataset_path, target_size=(224, 224), batch_size=32, class_mode='categorical')\n",
        "train_data, val_data = train_test_split(data, test_size=0.2, random_state=42)\n",
        "\n",
        "# Concatenate the train_data and val_data tuples\n",
        "train_x = np.concatenate([x for x, y in train_data])\n",
        "train_y = np.concatenate([y for x, y in train_data])\n",
        "val_x = np.concatenate([x for x, y in val_data])\n",
        "val_y = np.concatenate([y for x, y in val_data])\n",
        "\n",
        "# Define the VGG-Face model architecture\n",
        "model2 = Sequential()\n",
        "model2.add(Conv2D(64, (3, 3), activation='relu', padding='same', input_shape=(224, 224, 3)))\n",
        "model2.add(Conv2D(64, (3, 3), activation='relu', padding='same'))\n",
        "model2.add(MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "model2.add(Flatten())\n",
        "model2.add(Dense(512, activation='relu'))\n",
        "model2.add(Dropout(0.5))\n",
        "model2.add(Dense(19, activation='softmax', kernel_regularizer=regularizers.l2(0.015)))\n",
        "\n",
        "# Compile the model\n",
        "model2.compile(optimizer=optimizers.SGD(learning_rate=0.01, momentum=0.9), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Define a checkpoint to save the model weights after every epoch\n",
        "checkpoint = ModelCheckpoint('vgg_face_weights_{epoch:02d}.h5', save_weights_only=True)\n",
        "\n",
        "# Define early stopping callback\n",
        "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
        "\n",
        "# Train the model on the training data and validate on the validation data\n",
        "model2.fit(train_x, train_y, epochs=20, steps_per_epoch=len(train_x) // 32, validation_data=(val_x, val_y),\n",
        "           validation_steps=len(val_x) // 32, callbacks=[checkpoint, early_stop])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ZOizGh16kOgg"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\marey\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\engine\\training.py:3000: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        }
      ],
      "source": [
        "# Save the model\n",
        "model2.save('EVA2_ver5_model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Make predictions and save attendance information in real-time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Get the current working directory\n",
        "root_directory = os.getcwd()\n",
        "\n",
        "# Create a folder for attendance records\n",
        "attendance_folder = f\"{root_directory}/attendance\"\n",
        "os.makedirs(attendance_folder, exist_ok=True)\n",
        "\n",
        "# Create a new workbook object (Excel) for attendance records\n",
        "attendance_workbook = openpyxl.Workbook()\n",
        "tody_date = str(datetime.datetime.now().date())\n",
        "attendance_file_name = f\"{attendance_folder}/attendance_{tody_date}.xlsx\"\n",
        "\n",
        "# Select the active worksheet\n",
        "attendance_sheet = attendance_workbook.active"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the trained model \n",
        "model = load_model(r\"C:\\Users\\marey\\OneDrive\\Documents\\Test_22\\EVA2_ver5_model.h5\")\n",
        "\n",
        "# Create a face detection object\n",
        "detector = MTCNN()\n",
        "\n",
        "# Define the threshold for recognizing a student\n",
        "threshold = 0.4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1/1 [==============================] - 0s 312ms/step\n",
            "1/1 [==============================] - 0s 95ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "2/2 [==============================] - 0s 6ms/step\n",
            "1/1 [==============================] - 0s 114ms/step\n",
            "1/1 [==============================] - 0s 273ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 154ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 150ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 36ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 166ms/step\n",
            "1/1 [==============================] - 0s 42ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 155ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 149ms/step\n",
            "1/1 [==============================] - 0s 32ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 156ms/step\n",
            "1/1 [==============================] - 0s 34ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 24ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 152ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 26ms/step\n",
            "1/1 [==============================] - 0s 160ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 147ms/step\n",
            "1/1 [==============================] - 0s 35ms/step\n",
            "1/1 [==============================] - 0s 28ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 19ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 17ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 148ms/step\n",
            "1/1 [==============================] - 0s 33ms/step\n",
            "1/1 [==============================] - 0s 25ms/step\n",
            "1/1 [==============================] - 0s 21ms/step\n",
            "1/1 [==============================] - 0s 22ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 23ms/step\n",
            "1/1 [==============================] - 0s 18ms/step\n",
            "1/1 [==============================] - 0s 20ms/step\n",
            "1/1 [==============================] - 0s 27ms/step\n",
            "1/1 [==============================] - 0s 153ms/step\n"
          ]
        }
      ],
      "source": [
        "# Add some data to the worksheet\n",
        "attendance_sheet[\"A1\"] = \"Photo\"\n",
        "attendance_sheet[\"B1\"] = \"Name\"\n",
        "attendance_sheet[\"C1\"] = \"Time\"\n",
        "excel_row = 2\n",
        "handle_Repetition = {}\n",
        "\n",
        "\n",
        "# Initialize the video capture object\n",
        "cap = cv2.VideoCapture(0)\n",
        "\n",
        "pred_name = \"\"\n",
        "face_region = None\n",
        "\n",
        "# Process the video stream frame by frame\n",
        "while True:\n",
        "    # Capture a frame from the video stream\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    detections = detector.detect_faces(frame)\n",
        "\n",
        "    # Process each detected face\n",
        "    for detection in detections:\n",
        "        # Extract the face region from the frame\n",
        "        x, y, w, h = detection['box']\n",
        "        face_region = frame[y:y+h, x:x+w]\n",
        "\n",
        "        # Preprocess the face region\n",
        "        face_image = cv2.resize(face_region, (224, 224))\n",
        "        face_image = cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB)\n",
        "        face_image = image.img_to_array(face_image)\n",
        "        face_image = np.expand_dims(face_image, axis=0)\n",
        "        face_image = face_image / 255.0\n",
        "\n",
        "        # Pass the preprocessed face region through the trained model to obtain the predicted class probabilities\n",
        "        class_probabilities = model.predict(face_image)\n",
        "\n",
        "        # Identify the person in the frame if the predicted class probability is above the threshold\n",
        "        max_probability = np.max(class_probabilities)\n",
        "        if max_probability > threshold:\n",
        "            class_index = np.argmax(class_probabilities)\n",
        "            pred_name = class_labels[class_index] \n",
        "        else:\n",
        "            pred_name = \"Unknown\"\n",
        "\n",
        "        # Draw a bounding box and label around the face\n",
        "        cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
        "        cv2.putText(frame, pred_name, (x, y-10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
        "\n",
        "    # Display the processed frame\n",
        "    cv2.imshow('frame', frame)\n",
        "\n",
        "    # Exit if the user presses 'q'\n",
        "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "# Save the attendance information in an Excel file\n",
        "if pred_name not in handle_Repetition and face_region is not None:\n",
        "    # Get the current time\n",
        "    time = datetime.datetime.now().time()\n",
        "\n",
        "    # Save the attender's photo in the attendance folder\n",
        "    attender_photo_path = f'{attendance_folder}/{pred_name}.jpg'\n",
        "    cv2.imwrite(attender_photo_path, face_region)\n",
        "\n",
        "    # Write the attendance information in the Excel worksheet\n",
        "    attendance_sheet[\"A\" + str(excel_row)] = attender_photo_path\n",
        "    attendance_sheet[\"B\" + str(excel_row)] = pred_name\n",
        "    attendance_sheet[\"C\" + str(excel_row)] = time\n",
        "\n",
        "    # Update the number of times the person has been recognized\n",
        "    handle_Repetition[pred_name] = 1\n",
        "    excel_row += 1\n",
        "    attendance_workbook.save(attendance_file_name)\n",
        "\n",
        "# Release the video capture object and close the display window\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
